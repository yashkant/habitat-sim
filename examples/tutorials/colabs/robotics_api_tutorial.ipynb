{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habitat-sim Articulated Object API\n",
    "\n",
    "This tutorial covers the articulated object API in Habitat-sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Installation { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "!curl -L https://raw.githubusercontent.com/facebookresearch/habitat-sim/ao-api-tutorial/examples/colab_utils/colab_install.sh | PACKAGE=habitat-sim-ao bash -s\n",
    "!wget -c https://dl.fbaipublicfiles.com/habitat/URDF_demo_assets.zip && unzip -o URDF_demo_assets.zip -d /content/habitat-sim/data/URDF_demo_assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Path Setup and Imports { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "%cd /content/habitat-sim\n",
    "## [setup]\n",
    "# import math\n",
    "import os\n",
    "\n",
    "# import random\n",
    "import sys\n",
    "\n",
    "import git\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as ut\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "%matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "try:\n",
    "    # import ipywidgets as widgets\n",
    "    # from IPython.display import display as ipydisplay\n",
    "\n",
    "    # For using jupyter/ipywidget IO components\n",
    "\n",
    "    HAS_WIDGETS = True\n",
    "except ImportError:\n",
    "    HAS_WIDGETS = False\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "%cd $dir_path\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "output_directory = (\n",
    "    \"examples/tutorials/robotics_api_tutorial_output/\"  # @param {type:\"string\"}\n",
    ")\n",
    "output_path = os.path.join(dir_path, output_directory)\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "# define some globals the first time we run.\n",
    "if \"sim\" not in globals():\n",
    "    global sim\n",
    "    sim = None\n",
    "    global obj_attr_mgr\n",
    "    obj_attr_mgr = None\n",
    "    global prim_attr_mgr\n",
    "    obj_attr_mgr = None\n",
    "    global stage_attr_mgr\n",
    "    stage_attr_mgr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Simulation Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown These utility functions abstract away initial Simulator setup/configuration and repetative operations which would otherwise be duplicated:\n",
    "\n",
    "# @markdown - remove_all_objects\n",
    "def remove_all_objects(sim):\n",
    "    for ob_id in sim.get_existing_object_ids():\n",
    "        sim.remove_object(ob_id)\n",
    "    for ob_id in sim.get_existing_articulated_object_ids():\n",
    "        sim.remove_articulated_object(ob_id)\n",
    "\n",
    "\n",
    "# @markdown - simulate\n",
    "def simulate(sim, dt=1.0, get_frames=True):\n",
    "    # simulate dt seconds at 60Hz to the nearest fixed timestep\n",
    "    print(\"Simulating \" + str(dt) + \" world seconds.\")\n",
    "    observations = []\n",
    "    start_time = sim.get_world_time()\n",
    "    while sim.get_world_time() < start_time + dt:\n",
    "        sim.step_physics(1.0 / 60.0)\n",
    "        if get_frames:\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "\n",
    "    return observations\n",
    "\n",
    "\n",
    "# @markdown - place_robot_from_agent\n",
    "def place_robot_from_agent(\n",
    "    sim,\n",
    "    robot_id,\n",
    "    angle_correction=-1.56,\n",
    "    local_base_pos=None,\n",
    "):\n",
    "    if local_base_pos is None:\n",
    "        local_base_pos = np.array([0.0, -0.1, -2.0])\n",
    "    # place the robot root state relative to the agent\n",
    "    agent_transform = sim.agents[0].scene_node.transformation_matrix()\n",
    "    base_transform = mn.Matrix4.rotation(\n",
    "        mn.Rad(angle_correction), mn.Vector3(1.0, 0, 0)\n",
    "    )\n",
    "    base_transform.translation = agent_transform.transform_point(local_base_pos)\n",
    "    sim.set_articulated_object_root_state(robot_id, base_transform)\n",
    "\n",
    "\n",
    "# @markdown - place_camera\n",
    "def place_camera(\n",
    "    sim, pos=np.array([-0.15, -0.1, 1.0]), rot=np.quaternion(-0.83147, 0, 0.55557, 0)\n",
    "):\n",
    "    # place our \"camera person\" agent in the scene\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state.position = pos\n",
    "    agent_state.rotation = rot\n",
    "    agent = sim.initialize_agent(0, agent_state)\n",
    "    return agent.scene_node.transformation_matrix()\n",
    "\n",
    "\n",
    "# @markdown - make_default_settings\n",
    "def make_default_settings():\n",
    "    settings = {\n",
    "        \"width\": 720,  # Spatial resolution of the observations\n",
    "        \"height\": 544,\n",
    "        \"scene\": \"data/scene_datasets/habitat-test-scenes/apartment_1.glb\",  # Scene path\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "\n",
    "# @markdown - make_configuration\n",
    "def make_configuration(sim_settings):\n",
    "    # simulator configuration\n",
    "    backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    backend_cfg.scene_id = sim_settings[\"scene\"]\n",
    "    backend_cfg.enable_physics = True\n",
    "\n",
    "    # configure rgbd sensors coincident with \"agent\" position/orientation\n",
    "    camera_resolution = [sim_settings[\"height\"], sim_settings[\"width\"]]\n",
    "    sensors = {\n",
    "        \"rgba_camera_1stperson\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.COLOR,\n",
    "            \"resolution\": camera_resolution,\n",
    "            \"position\": [0.0, 0.0, 0.0],\n",
    "            \"orientation\": [0.0, 0.0, 0.0],\n",
    "        },\n",
    "        \"depth_camera_1stperson\": {\n",
    "            \"sensor_type\": habitat_sim.SensorType.DEPTH,\n",
    "            \"resolution\": camera_resolution,\n",
    "            \"position\": [0.0, 0.0, 0.0],\n",
    "            \"orientation\": [0.0, 0.0, 0.0],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    sensor_specs = []\n",
    "    for sensor_uuid, sensor_params in sensors.items():\n",
    "        sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "        sensor_spec.uuid = sensor_uuid\n",
    "        sensor_spec.sensor_type = sensor_params[\"sensor_type\"]\n",
    "        sensor_spec.resolution = sensor_params[\"resolution\"]\n",
    "        sensor_spec.position = sensor_params[\"position\"]\n",
    "        sensor_spec.orientation = sensor_params[\"orientation\"]\n",
    "        sensor_specs.append(sensor_spec)\n",
    "\n",
    "    # agent \"camera person\" configuration\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "\n",
    "    return habitat_sim.Configuration(backend_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "# @markdown - remake_simulator\n",
    "def remake_simulator(sim_settings):\n",
    "    cfg = make_configuration(sim_settings)\n",
    "    # clean-up the current simulator instance if it exists\n",
    "    global sim\n",
    "    global obj_attr_mgr\n",
    "    global prim_attr_mgr\n",
    "    global stage_attr_mgr\n",
    "    if sim != None:\n",
    "        # close the simulator to clear allocated asset memory\n",
    "        # Note: force destruction of the background rendering thread\n",
    "        sim.close(destroy=True)\n",
    "    # initialize the simulator\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    # Managers of various Attributes templates\n",
    "    obj_attr_mgr = sim.get_object_template_manager()\n",
    "    obj_attr_mgr.load_configs(str(os.path.join(data_path, \"objects\")))\n",
    "    prim_attr_mgr = sim.get_asset_template_manager()\n",
    "    stage_attr_mgr = sim.get_stage_template_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Pre-define Paths to Example URDF Files Available for Import:\n",
    "# @markdown  - Aliengo\n",
    "# @markdown  - KUKA Iiwa\n",
    "# @markdown  - Locobot (w/ and w/o gripper)\n",
    "\n",
    "urdf_files = {\n",
    "    \"aliengo\": os.path.join(data_path, \"URDF_demo_assets/aliengo/urdf/aliengo.urdf\"),\n",
    "    \"iiwa\": os.path.join(data_path, \"test_assets/URDF/kuka_iiwa/model_free_base.urdf\"),\n",
    "    \"locobot\": os.path.join(data_path, \"URDF_demo_assets/aliengo/urdf/aliengo.urdf\"),\n",
    "    \"locobot_light\": os.path.join(\n",
    "        data_path, \"URDF_demo_assets/aliengo/urdf/aliengo.urdf\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Visualization Flags { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    make_video = False\n",
    "    display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title A Minimal Example\n",
    "\n",
    "# @markdown Load a robot URDF into a scene and simulate.\n",
    "def minimal_urdf_example(make_video=True, show_video=True):\n",
    "\n",
    "    # create a fresh Simulator instance\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "\n",
    "    # position the camera\n",
    "    camera_tilt = ut.quat_from_angle_axis(-0.4, np.array([1.0, 0, 0]))\n",
    "    camera_pan = ut.quat_from_angle_axis(-1.12, np.array([0, 1.0, 0]))\n",
    "    place_camera(sim, rot=camera_pan * camera_tilt)\n",
    "\n",
    "    # load a URDF file\n",
    "    robot_file = urdf_files[\"aliengo\"]\n",
    "    robot_id = sim.add_articulated_object_from_urdf(robot_file)\n",
    "\n",
    "    # place the robot root state relative to the agent\n",
    "    place_robot_from_agent(sim, robot_id)\n",
    "\n",
    "    # simulate\n",
    "    observations = simulate(sim, dt=2.0, get_frames=make_video)\n",
    "\n",
    "    if make_video:\n",
    "        vut.make_video(\n",
    "            observations,\n",
    "            \"rgba_camera_1stperson\",\n",
    "            \"color\",\n",
    "            output_path + \"minimal_example\",\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "\n",
    "minimal_urdf_example(make_video, show_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Get/set Robot State:\n",
    "\n",
    "\n",
    "def robot_state_manipulation(make_video=True, show_video=True):\n",
    "    # create a fresh Simulator instance\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "\n",
    "    # position the camera\n",
    "    camera_tilt = ut.quat_from_angle_axis(-0.4, np.array([1.0, 0, 0]))\n",
    "    camera_pan = ut.quat_from_angle_axis(-1.12, np.array([0, 1.0, 0]))\n",
    "    place_camera(sim, rot=camera_pan * camera_tilt)\n",
    "\n",
    "    # load a URDF file\n",
    "    robot_file = urdf_files[\"aliengo\"]\n",
    "    robot_id = sim.add_articulated_object_from_urdf(robot_file)\n",
    "\n",
    "    # place the robot root state relative to the agent\n",
    "    place_robot_from_agent(sim, robot_id)\n",
    "\n",
    "    # @markdown Getting and setting torques, velocities, and positions.\n",
    "    # @markdown - `get_articulated_object_forces(robot_id)`\n",
    "    # @markdown - `set_articulated_object_forces(robot_id, tau)`\n",
    "    # @markdown - `get_articulated_object_velocities(robot_id)`\n",
    "    # @markdown - `set_articulated_object_velocities(robot_id, q_dot)`\n",
    "    # @markdown - `get_articulated_object_positions(robot_id)`\n",
    "    # @markdown - `set_articualted_object_positions(robot_id, q)`\n",
    "    tau = sim.get_articulated_object_forces(robot_id)\n",
    "    sim.set_articulated_object_forces(robot_id, tau)\n",
    "\n",
    "    vel = sim.get_articulated_object_velocities(robot_id)\n",
    "    sim.set_articulated_object_velocities(robot_id, vel)\n",
    "\n",
    "    pos = sim.get_articulated_object_positions(robot_id)\n",
    "    sim.set_articulated_object_positions(robot_id, pos)\n",
    "\n",
    "    observations = []\n",
    "    observations += simulate(sim, dt=1.0, get_frames=make_video)\n",
    "\n",
    "    # @markdown Setting MotionType: a KINEMATIC articulated oject is collidable, but does not actively simulate dynamics.\n",
    "    # @markdown - `set_articulated_object_motion_type(robot_id, MotionType)`\n",
    "    # @markdown - `get_articulated_object_motion_type(robot_id)`\n",
    "    sim.set_articulated_object_motion_type(\n",
    "        robot_id, habitat_sim.physics.MotionType.KINEMATIC\n",
    "    )\n",
    "    assert (\n",
    "        sim.get_articulated_object_motion_type(robot_id)\n",
    "        == habitat_sim.physics.MotionType.KINEMATIC\n",
    "    )\n",
    "\n",
    "    # @markdown `reset_articulated_object(robot_id)`: resets the positions, velocities, and torques. Then computes forward kinematics and updates the collision state.\n",
    "    sim.reset_articulated_object(robot_id)\n",
    "    # @markdown - note: reset does not change the robot base state, do this manually with `set_articulated_object_root_state(robot_id, base_transform)`.\n",
    "    place_robot_from_agent(sim, robot_id)\n",
    "\n",
    "    # @markdown Querying link states and maximal coordinates.\n",
    "    # @markdown - `get_articulated_link_rigid_state(robot_id, link_id)`\n",
    "    # @markdown - `get_articulated_link_friction(robot_id, link_id)`\n",
    "    # get rigid state of robot links and show proxy object at each link COM\n",
    "    obj_mgr = sim.get_object_template_manager()\n",
    "    cube_id = sim.add_object_by_handle(obj_mgr.get_template_handles(\"cube\")[0])\n",
    "    sim.set_object_motion_type(habitat_sim.physics.MotionType.KINEMATIC, cube_id)\n",
    "    sim.set_object_is_collidable(False, cube_id)\n",
    "    num_links = sim.get_num_articulated_links(robot_id)\n",
    "    for link_id in range(num_links):\n",
    "        link_rigid_state = sim.get_articulated_link_rigid_state(robot_id, link_id)\n",
    "        sim.set_translation(link_rigid_state.translation, cube_id)\n",
    "        sim.set_rotation(link_rigid_state.rotation, cube_id)\n",
    "        # get the link friction\n",
    "        print(\n",
    "            \"Link \"\n",
    "            + str(link_id)\n",
    "            + \" friction coefficient = \"\n",
    "            + str(sim.get_articulated_link_friction(robot_id, link_id))\n",
    "        )\n",
    "        # Note: set this with 'sim.get_articulated_link_friction(robot_id, link_id, friction)'\n",
    "        observations += simulate(sim, dt=0.5, get_frames=make_video)\n",
    "    sim.remove_object(cube_id)\n",
    "\n",
    "    sim.set_articulated_object_motion_type(\n",
    "        robot_id, habitat_sim.physics.MotionType.DYNAMIC\n",
    "    )\n",
    "    assert (\n",
    "        sim.get_articulated_object_motion_type(robot_id)\n",
    "        == habitat_sim.physics.MotionType.DYNAMIC\n",
    "    )\n",
    "\n",
    "    if make_video:\n",
    "        vut.make_video(\n",
    "            observations,\n",
    "            \"rgba_camera_1stperson\",\n",
    "            \"color\",\n",
    "            output_path + \"URDF_basics\",\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "\n",
    "robot_state_manipulation(make_video, show_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Joint Motor Control\n",
    "\n",
    "# @markdown Configuring position and velocity motors for robot joints.\n",
    "\n",
    "\n",
    "def joint_motors(make_video=True, show_video=True):\n",
    "    # create a fresh Simulator instance\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "\n",
    "    # position the camera\n",
    "    place_camera(sim)\n",
    "    observations = []\n",
    "\n",
    "    # load a URDF file with a fixed base\n",
    "    robot_file = urdf_files[\"iiwa\"]\n",
    "    robot_id = sim.add_articulated_object_from_urdf(robot_file, True)\n",
    "\n",
    "    # place the robot root state relative to the agent\n",
    "    place_robot_from_agent(sim, robot_id, -3.14)\n",
    "\n",
    "    # @markdown  - By default, joint damping values from the URDF are consumed as max_impulse for a set of velocity JointMotors (with gain=1).\n",
    "    # @markdown  - `get_existing_joint_motors(robot_id)` - JointMotors can be queried for a robot returning `[motor_id, dof]` pairs.\n",
    "\n",
    "    # query any damping motors created by default\n",
    "    existing_joint_motors = sim.get_existing_joint_motors(robot_id)\n",
    "    print(\"Default damping motors (motor_id -> dof): \" + str(existing_joint_motors))\n",
    "\n",
    "    # get the max_impulse of the damping motors\n",
    "    for motor_id in existing_joint_motors:\n",
    "        motor_settings = sim.get_joint_motor_settings(robot_id, motor_id)\n",
    "        print(\n",
    "            \"   motor(\"\n",
    "            + str(motor_id)\n",
    "            + \"): max_impulse = \"\n",
    "            + str(motor_settings.max_impulse)\n",
    "        )\n",
    "\n",
    "    # simulate\n",
    "    observations += simulate(sim, dt=1.5, get_frames=make_video)\n",
    "\n",
    "    # @markdown  - `create_joint_motor(robot_id, dof, settings)` - New JointMotors can be created from JointMotorSettings for Revolute and Prismatic joints.\n",
    "    # create a new velocity motor\n",
    "    joint_motor_settings = habitat_sim.physics.JointMotorSettings(\n",
    "        0,  # position_target\n",
    "        0,  # position_gain\n",
    "        1.0,  # velocity_target\n",
    "        1.0,  # velocity_gain\n",
    "        10.0,  # max_impulse\n",
    "    )\n",
    "    new_motor_id = sim.create_joint_motor(\n",
    "        robot_id, 1, joint_motor_settings  # robot object id  # dof  # settings\n",
    "    )\n",
    "    existing_joint_motors = sim.get_existing_joint_motors(robot_id)\n",
    "    print(\"new_motor_id: \" + str(new_motor_id))\n",
    "    print(\n",
    "        \"Existing motors after create (motor_id -> dof): \" + str(existing_joint_motors)\n",
    "    )\n",
    "\n",
    "    # simulate\n",
    "    observations += simulate(sim, dt=1.5, get_frames=make_video)\n",
    "\n",
    "    # @markdown  - `update_joint_motor(robot_id, motor_id, settings)` - JointMotors can be updated from JointMotorSettings.\n",
    "    # reverse the motor velocity\n",
    "    joint_motor_settings.velocity_target = -1.0\n",
    "    sim.update_joint_motor(robot_id, new_motor_id, joint_motor_settings)\n",
    "\n",
    "    # simulate\n",
    "    observations += simulate(sim, dt=1.5, get_frames=make_video)\n",
    "\n",
    "    # @markdown  - `remove_joint_motor(robot_id, motor_id)` - JointMotors can be removed by id.\n",
    "    # remove the new joint motor\n",
    "    sim.remove_joint_motor(robot_id, new_motor_id)\n",
    "\n",
    "    # @markdown  - `create_motors_for_all_dofs(robot_id, (optional)settings)` - JointMotors can be created for all of a robot's joints (Revolute|Prismatic) from a single JointMotorSettings. Returns `[[dof, motor_id], ...]`.\n",
    "    # create joint motors for all valid dofs to control a pose (1.1 for all dofs)\n",
    "    joint_motor_settings = habitat_sim.physics.JointMotorSettings(0.5, 1.0, 0, 0, 1.0)\n",
    "    dofs_to_motor_ids = sim.create_motors_for_all_dofs(\n",
    "        robot_id,\n",
    "        joint_motor_settings,  # (optional) motor settings, if not provided will be default (no gains)\n",
    "    )\n",
    "    print(\"New motors (motor_ids -> dofs): \" + str(dofs_to_motor_ids))\n",
    "\n",
    "    # simulate\n",
    "    observations += simulate(sim, dt=2.0, get_frames=make_video)\n",
    "\n",
    "    # remove all motors\n",
    "    existing_joint_motors = sim.get_existing_joint_motors(robot_id)\n",
    "    print(\"All motors (motor_id -> dof) before removal: \" + str(existing_joint_motors))\n",
    "    for motor_id in existing_joint_motors:\n",
    "        sim.remove_joint_motor(robot_id, motor_id)\n",
    "    print(\n",
    "        \"All motors (motor_id -> dof) before removal: \"\n",
    "        + str(sim.get_existing_joint_motors(robot_id))\n",
    "    )\n",
    "\n",
    "    # simulate\n",
    "    observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "\n",
    "    if make_video:\n",
    "        vut.make_video(\n",
    "            observations,\n",
    "            \"rgba_camera_1stperson\",\n",
    "            \"color\",\n",
    "            output_path + \"URDF_joint_motors\",\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "\n",
    "joint_motors(make_video, show_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Scaling URDF and Caching\n",
    "\n",
    "# @markdown Loaded URDF files are cached and future load calls import the cached version unless `force_reload=True` is specified:\n",
    "# @markdown `add_articulated_object_from_urdf(URDF_file, use_fixed_base=False, urdf_global_scale=1, mass_scale=1, force_reload=False)`\n",
    "# @markdown Note: A cached model can be re-scaled without re-parsing the file.\n",
    "def urdf_scaling(make_video=True, show_video=True):\n",
    "    # create a fresh Simulator instance\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "\n",
    "    # position the camera\n",
    "    camera_tilt = ut.quat_from_angle_axis(-0.3, np.array([1.0, 0, 0]))\n",
    "    camera_pan = ut.quat_from_angle_axis(-1.12, np.array([0, 1.0, 0]))\n",
    "    place_camera(sim, rot=camera_pan * camera_tilt)\n",
    "\n",
    "    observations = []\n",
    "    for iteration in range(1, 4):\n",
    "        # load a URDF file with variable scale\n",
    "        robot_file = urdf_files[\"aliengo\"]\n",
    "        urdf_global_scale = iteration / 2.0\n",
    "        robot_id = sim.add_articulated_object_from_urdf(\n",
    "            robot_file, False, urdf_global_scale\n",
    "        )\n",
    "        print(\"Scaled URDF by \" + str(urdf_global_scale))\n",
    "\n",
    "        # place the robot root state relative to the agent\n",
    "        place_robot_from_agent(sim, robot_id)\n",
    "\n",
    "        # set a better initial joint state for the aliengo\n",
    "        if robot_file == urdf_files[\"aliengo\"]:\n",
    "            pose = sim.get_articulated_object_positions(robot_id)\n",
    "            calfDofs = [2, 5, 8, 11]\n",
    "            for dof in calfDofs:\n",
    "                pose[dof] = -1.0\n",
    "                pose[dof - 1] = 0.45\n",
    "                # also set a thigh\n",
    "            sim.set_articulated_object_positions(robot_id, pose)\n",
    "\n",
    "        # simulate\n",
    "        observations += simulate(sim, dt=1.5, get_frames=make_video)\n",
    "\n",
    "        # clear all robots\n",
    "        # for robot_id in sim.get_existing_articulated_object_ids():\n",
    "        #  sim.remove_articulated_object(robot_id)\n",
    "\n",
    "    if make_video:\n",
    "        vut.make_video(\n",
    "            observations,\n",
    "            \"rgba_camera_1stperson\",\n",
    "            \"color\",\n",
    "            output_path + \"URDF_scaling\",\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "\n",
    "urdf_scaling(make_video, show_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Creating and Managing Constraints\n",
    "# @markdown Habitat-sim provides APIs for creating point-to-point (ball joint) constraints between any combination of articulated and rigid objects.\n",
    "# @markdown *Note: all constraint creation functions allow an optional final variable* `max_impulse=2` *to configure the constraint strength.*\n",
    "\n",
    "\n",
    "def test_constraints(make_video=True, show_video=True, test_case=0):\n",
    "    # create the simulator\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "    place_camera(sim)\n",
    "    observations = []\n",
    "\n",
    "    # load a URDF file\n",
    "    robot_file = urdf_files[\"aliengo\"]\n",
    "    robot_id = sim.add_articulated_object_from_urdf(robot_file)\n",
    "    ef_link_id = 16  # foot = 16, TODO: base = -1\n",
    "    ef_link2_id = 12\n",
    "    iiwa_ef_link = 6\n",
    "\n",
    "    # add a constraint visualization object\n",
    "    obj_mgr = sim.get_object_template_manager()\n",
    "    sphere_id = sim.add_object_by_handle(obj_mgr.get_template_handles(\"sphere\")[0])\n",
    "    sim.set_object_motion_type(habitat_sim.physics.MotionType.KINEMATIC, sphere_id)\n",
    "    sim.set_object_is_collidable(False, sphere_id)\n",
    "\n",
    "    place_robot_from_agent(sim, robot_id)\n",
    "\n",
    "    # Test constraint types:\n",
    "    if test_case == 0:\n",
    "        # @markdown **AO -> world**\n",
    "        # should constrain to the center of the sphere\n",
    "        link_rigid_state = sim.get_articulated_link_rigid_state(robot_id, ef_link_id)\n",
    "        sim.set_translation(link_rigid_state.translation, sphere_id)\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot_id, link_id, global_point)`: constraint a link to a global point. The pivot is located at the constraint position.\n",
    "        constraint_id = sim.create_articulated_p2p_constraint(\n",
    "            object_id=robot_id,\n",
    "            link_id=ef_link_id,\n",
    "            global_constraint_point=link_rigid_state.translation,\n",
    "        )\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "    elif test_case == 1:\n",
    "        # - AO -> world w/ offset\n",
    "        # should constrain to the boundary of the sphere\n",
    "        link_rigid_state = sim.get_articulated_link_rigid_state(robot_id, ef_link_id)\n",
    "        link_offset = mn.Vector3(0, 0, -0.1)\n",
    "        global_constraint_position = link_rigid_state.translation\n",
    "        sim.set_translation(global_constraint_position, sphere_id)\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot_id, link_id, link_offset, global_point)`: constraint a particular local point on the link to a global point.\n",
    "        constraint_id = sim.create_articulated_p2p_constraint(\n",
    "            object_id=robot_id,\n",
    "            link_id=ef_link_id,\n",
    "            link_offset=link_offset,\n",
    "            global_constraint_point=global_constraint_position,\n",
    "        )\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "    elif test_case == 2:\n",
    "        # @markdown **AO -> AO** (w/ offsets)\n",
    "        robot_id2 = sim.add_articulated_object_from_urdf(robot_file)\n",
    "        place_robot_from_agent(\n",
    "            sim=sim,\n",
    "            robot_id=robot_id2,\n",
    "            local_base_pos=np.array([0.35, -0.1, -2.0]),\n",
    "        )\n",
    "        # attach the agents' feet together\n",
    "        link_b_rigid_state = sim.get_articulated_link_rigid_state(robot_id2, ef_link_id)\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot1_id, link1_id, offset1, robot2_id, link2_id, offset2)`: constraint a local point on one articulated object to a local point on another.\n",
    "        constraint_id = sim.create_articulated_p2p_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id_a=ef_link_id,\n",
    "            offset_a=mn.Vector3(),\n",
    "            object_id_b=robot_id2,\n",
    "            link_id_b=ef_link_id,\n",
    "            offset_b=mn.Vector3(),\n",
    "        )\n",
    "\n",
    "        # constrain 1st robot in the air by other foot\n",
    "        link_a2_rigid_state = sim.get_articulated_link_rigid_state(\n",
    "            robot_id, ef_link2_id\n",
    "        )\n",
    "        global_constraint_position = link_a2_rigid_state.translation + mn.Vector3(\n",
    "            0, 1.5, 0\n",
    "        )\n",
    "        sim.set_translation(global_constraint_position, sphere_id)\n",
    "        # note: increase max impulse: the combined weight of the robots is greater than the default impulse correction (2)\n",
    "        constraint_id2 = sim.create_articulated_p2p_constraint(\n",
    "            object_id=robot_id,\n",
    "            link_id=ef_link2_id,\n",
    "            link_offset=mn.Vector3(),\n",
    "            global_constraint_point=global_constraint_position,\n",
    "            max_impulse=6,\n",
    "        )\n",
    "\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "        sim.remove_constraint(constraint_id2)\n",
    "        sim.remove_articulated_object(robot_id2)\n",
    "    elif test_case == 3:\n",
    "        # - AO -> AO (global)\n",
    "        robot_id2 = sim.add_articulated_object_from_urdf(urdf_files[\"iiwa\"], True)\n",
    "        place_robot_from_agent(\n",
    "            sim=sim,\n",
    "            robot_id=robot_id2,\n",
    "            local_base_pos=np.array([0.35, -0.4, -2.0]),\n",
    "        )\n",
    "        jm_settings = habitat_sim.physics.JointMotorSettings()\n",
    "        jm_settings.position_gain = 2.0\n",
    "        sim.create_motors_for_all_dofs(robot_id2, jm_settings)\n",
    "        # TODO: not a great test, could be a better setup\n",
    "        # attach two agent feet to the iiwa end effector\n",
    "        link_b_rigid_state = sim.get_articulated_link_rigid_state(\n",
    "            robot_id2, iiwa_ef_link\n",
    "        )\n",
    "        global_constraint_position = link_b_rigid_state.translation\n",
    "        sim.set_translation(global_constraint_position, sphere_id)\n",
    "        constraint_id = sim.create_articulated_p2p_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id_a=ef_link_id,\n",
    "            object_id_b=robot_id2,\n",
    "            link_id_b=iiwa_ef_link,\n",
    "            global_constraint_point=global_constraint_position,\n",
    "            max_impulse=4,\n",
    "        )\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot1_id, link1_id, robot2_id, link2_id, global_point)`: constrain two articulated object links at a global point.\n",
    "        constraint_id2 = sim.create_articulated_p2p_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id_a=ef_link2_id,\n",
    "            object_id_b=robot_id2,\n",
    "            link_id_b=iiwa_ef_link,\n",
    "            global_constraint_point=global_constraint_position,\n",
    "            max_impulse=4,\n",
    "        )\n",
    "\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "        sim.remove_constraint(constraint_id2)\n",
    "        sim.remove_articulated_object(robot_id2)\n",
    "    elif test_case == 4:\n",
    "        # @markdown **AO -> rigid**\n",
    "        # @markdown  - *Note: rigid->world is also provided `create_rigid_p2p_constraint` but not demonstrated here.*\n",
    "        # tilt the camera down\n",
    "        prev_state = sim.get_agent(0).scene_node.rotation\n",
    "        sim.get_agent(0).scene_node.rotation = (\n",
    "            mn.Quaternion.rotation(\n",
    "                mn.Rad(-0.4), prev_state.transform_vector(mn.Vector3(1.0, 0, 0))\n",
    "            )\n",
    "            * prev_state\n",
    "        )\n",
    "\n",
    "        # attach an active sphere to one robot foot w/ pivot at the object center\n",
    "        active_sphere_id = sim.add_object_by_handle(\n",
    "            obj_mgr.get_template_handles(\"sphere\")[0]\n",
    "        )\n",
    "        link_rigid_state = sim.get_articulated_link_rigid_state(robot_id, ef_link_id)\n",
    "        link2_rigid_state = sim.get_articulated_link_rigid_state(robot_id, ef_link2_id)\n",
    "        sim.set_translation(\n",
    "            link2_rigid_state.translation + mn.Vector3(0, -0.1, 0),\n",
    "            active_sphere_id,\n",
    "        )\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot_id, link_id, object_id)`: constrain an object to an articulated link with pivot at the object's COM.\n",
    "        constraint_id = sim.create_articulated_p2p_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id=ef_link2_id,\n",
    "            object_id_b=active_sphere_id,\n",
    "            max_impulse=4,\n",
    "        )\n",
    "        # attach the visual sphere to another robot foot w/ pivots\n",
    "        sim.set_object_motion_type(habitat_sim.physics.MotionType.DYNAMIC, sphere_id)\n",
    "        # @markdown - `create_articulated_p2p_constraint(robot_id, link_id, object_id, offset_link, offset_object)`: constrain an object to an articulated link with provided local pivots for each.\n",
    "        constraint_id2 = sim.create_articulated_p2p_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id=ef_link_id,\n",
    "            object_id_b=sphere_id,\n",
    "            pivot_a=mn.Vector3(0.1, 0, 0),\n",
    "            pivot_b=mn.Vector3(-0.1, 0, 0),\n",
    "            max_impulse=4,\n",
    "        )\n",
    "\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "        sim.remove_constraint(constraint_id2)\n",
    "        sim.set_object_motion_type(habitat_sim.physics.MotionType.KINEMATIC, sphere_id)\n",
    "        sim.remove_object(active_sphere_id)\n",
    "\n",
    "        sim.get_agent(0).scene_node.rotation = prev_state\n",
    "    elif test_case == 5:\n",
    "        # - AO -> rigid (fixed) TODO: not working as expected\n",
    "\n",
    "        # tilt the camera down\n",
    "        prev_state = sim.get_agent(0).scene_node.rotation\n",
    "        sim.get_agent(0).scene_node.rotation = (\n",
    "            mn.Quaternion.rotation(\n",
    "                mn.Rad(-0.4), prev_state.transform_vector(mn.Vector3(1.0, 0, 0))\n",
    "            )\n",
    "            * prev_state\n",
    "        )\n",
    "\n",
    "        # attach an active sphere to one robot foot w/ pivot at the object center\n",
    "        active_sphere_id = sim.add_object_by_handle(\n",
    "            obj_mgr.get_template_handles(\"sphere\")[0]\n",
    "        )\n",
    "        link2_rigid_state = sim.get_articulated_link_rigid_state(robot_id, ef_link2_id)\n",
    "        sim.set_translation(\n",
    "            link2_rigid_state.translation + mn.Vector3(0, -0.15, 0),\n",
    "            active_sphere_id,\n",
    "        )\n",
    "        constraint_id = sim.create_articulated_fixed_constraint(\n",
    "            object_id_a=robot_id,\n",
    "            link_id=ef_link2_id,\n",
    "            object_id_b=active_sphere_id,\n",
    "            max_impulse=4,\n",
    "        )\n",
    "\n",
    "        observations += simulate(sim, dt=3.0, get_frames=make_video)\n",
    "        sim.remove_constraint(constraint_id)\n",
    "        sim.remove_object(active_sphere_id)\n",
    "\n",
    "        sim.get_agent(0).scene_node.rotation = prev_state\n",
    "\n",
    "    if make_video:\n",
    "        vut.make_video(\n",
    "            observations,\n",
    "            \"rgba_camera_1stperson\",\n",
    "            \"color\",\n",
    "            output_path + \"test_constraints_\" + str(test_case),\n",
    "            open_vid=show_video,\n",
    "        )\n",
    "\n",
    "\n",
    "# @markdown **Constraints can be removed by ID** (returned from creation):\n",
    "# @markdown - `remove_constraint(constraint_id)`\n",
    "\n",
    "# run all constraint variants\n",
    "for test_case in range(5):\n",
    "    test_constraints(make_video, show_video, test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Contact information queries:\n",
    "\n",
    "# @markdown Habitat-sim provides several contact query features including:\n",
    "\n",
    "\n",
    "def demo_contact_profile():\n",
    "    sim_settings = make_default_settings()\n",
    "    remake_simulator(sim_settings)\n",
    "    place_camera(sim)\n",
    "    observations = []\n",
    "\n",
    "    # add a robot to the scene\n",
    "    robot_file = urdf_files[\"aliengo\"]\n",
    "    robot_id = sim.add_articulated_object_from_urdf(robot_file)\n",
    "\n",
    "    # @markdown - `get_physics_step_collision_summary()`: returns a summary string of the previous collision detection iteration.\n",
    "    # gets nothing because physics has not stepped yet\n",
    "    print(sim.get_physics_step_collision_summary())\n",
    "\n",
    "    # gets nothing because no active collisions yet\n",
    "    sim.step_physics(0.1)\n",
    "    print(sim.get_physics_step_collision_summary())\n",
    "\n",
    "    # give time for the robot to hit the ground then check contacts\n",
    "    sim.step_physics(1.0)\n",
    "    print(\"Step Collision Summary:\")\n",
    "    print(sim.get_physics_step_collision_summary())\n",
    "\n",
    "    # now add two colliding robots and run discrete collision detection\n",
    "    sim.remove_articulated_object(robot_id)\n",
    "    robot_id1 = sim.add_articulated_object_from_urdf(robot_file)\n",
    "    robot_id2 = sim.add_articulated_object_from_urdf(robot_file)\n",
    "    place_robot_from_agent(sim, robot_id1)\n",
    "    place_robot_from_agent(sim, robot_id2, local_base_pos=np.array([0.15, -0.1, -2.0]))\n",
    "    # @markdown - `perform_discrete_collision_detection()`: performs a single discrete collision check for the full scene.\n",
    "    sim.perform_discrete_collision_detection()\n",
    "    print(\"Step Collision Summary:\")\n",
    "    print(sim.get_physics_step_collision_summary())\n",
    "    # @markdown - `get_physics_num_active_overlapping_pairs()`: returns the number of \"active\" overlapping collision object pairs found during the previous collision check.\n",
    "    print(\n",
    "        \"Num overlapping pairs: \" + str(sim.get_physics_num_active_overlapping_pairs())\n",
    "    )\n",
    "    # @markdown - `get_physics_num_active_contact_points()`: returns the number of \"active\" contact points found during the previous collision check.\n",
    "    print(\n",
    "        \"Num active contact points: \" + str(sim.get_physics_num_active_contact_points())\n",
    "    )\n",
    "    # @markdown - `get_physics_contact_points()`: returns an information structure for all active contact points.\n",
    "    contact_points = sim.get_physics_contact_points()\n",
    "    print(\"Active contact points: \")\n",
    "    for cp_ix, cp in enumerate(contact_points):\n",
    "        print(\" Contact Point \" + str(cp_ix) + \":\")\n",
    "        print(\"     object_id_a = \" + str(cp.object_id_a))\n",
    "        print(\"     object_id_b = \" + str(cp.object_id_b))\n",
    "        print(\"     link_id_a = \" + str(cp.link_id_a))\n",
    "        print(\"     link_id_b = \" + str(cp.link_id_b))\n",
    "        print(\"     position_on_a_in_ws = \" + str(cp.position_on_a_in_ws))\n",
    "        print(\"     position_on_b_in_ws = \" + str(cp.position_on_b_in_ws))\n",
    "        print(\"     contact_normal_on_b_in_ws = \" + str(cp.contact_normal_on_b_in_ws))\n",
    "        print(\"     contact_distance = \" + str(cp.contact_distance))\n",
    "        print(\"     normal_force = \" + str(cp.normal_force))\n",
    "        print(\"     linear_friction_force1 = \" + str(cp.linear_friction_force1))\n",
    "        print(\"     linear_friction_force2 = \" + str(cp.linear_friction_force2))\n",
    "        print(\"     linear_friction_direction1 = \" + str(cp.linear_friction_direction1))\n",
    "        print(\"     linear_friction_direction2 = \" + str(cp.linear_friction_direction2))\n",
    "        print(\"     is_active = \" + str(cp.is_active))\n",
    "\n",
    "    observations.append(sim.get_sensor_observations())\n",
    "    # TODO: visualize the contact points\n",
    "    im = vut.observation_to_image(observations[-1][\"rgba_camera_1stperson\"], \"color\")\n",
    "    im.show()\n",
    "\n",
    "\n",
    "demo_contact_profile()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "robotics_api_tutorial.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,colabs//ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
